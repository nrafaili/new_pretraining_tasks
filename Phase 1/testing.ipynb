{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import EsmConfig, EsmForMaskedLM, EsmTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, EarlyStoppingCallback, EvalPrediction\n",
    "from transformers.data.data_collator import DataCollatorMixin, _torch_collate_batch\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from transformers.trainer import has_length\n",
    "from transformers.utils import is_datasets_available\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler, RandomSampler\n",
    "import argparse\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data arguments\n",
    "    parser.add_argument('--data_path', type=str, required=True, help='Dataset path')\n",
    "    parser.add_argument('--tokenizer_path', type=str, required=True, help='Tokenizer path')\n",
    "    parser.add_argument('--save_path', type=str, default='./best_model', help='Save Path')\n",
    "    parser.add_argument('--weight_path', type=str, default='./best_model', help='Weight Save Path')\n",
    "\n",
    "    # Model arguments\n",
    "    parser.add_argument('--hidden_size', type=int, default=512, help='Embedding dimension')\n",
    "    parser.add_argument('--num_hidden_layers', type=int, default=24, help='Number of transformer blocks')\n",
    "    parser.add_argument('--seed', type=int, default=338, help='Random seed for reproducibility')\n",
    "    parser.add_argument('--model_path', type=str, required=True, help='Model path')\n",
    "\n",
    "    # Training arguments\n",
    "    parser.add_argument('--wandb', action='store_true', help='Use Weights & Biases for logging')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "    parser.add_argument('--num_epochs', type=int, default=1, help='Number of epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--mlm_probability', type=str, choices=['0.05', '0.15', '0.3', '0.5', 'mixed'], default='mixed',\n",
    "                        help='MLM probability')\n",
    "    parser.add_argument('--patience', type=int, default=3, help='Early stopping patience')\n",
    "    parser.add_argument('--group_by_length', action='store_true', default=True, help='Group by length for SortedTrainer')\n",
    "    parser.add_argument('--grad_accum', type=int, default=1, help='Gradient Accumulation')\n",
    "    parser.add_argument('--evaluation_strategy', type=str, default='steps', help='Evaluation strategy to use')\n",
    "    parser.add_argument('--lr_scheduler_type', type=str, default='cosine', help='Type of learning rate scheduler')\n",
    "    parser.add_argument('--optim', type=str, default='adamw_torch', help='Optimizer')\n",
    "    parser.add_argument('--log_path', type=str, help='Path for logging')\n",
    "    parser.add_argument('--logging_steps', type=int, default=10, help='Step interval for logging')\n",
    "    parser.add_argument('--eval_steps', type=int, default=50, help='Step interval for evaluation')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay rate')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=10, help='Number of warmup steps')\n",
    "    parser.add_argument('--effective_batch_size', type=int, default=1000000, help='Effective batch size for training')\n",
    "    parser.add_argument('--save_total_limit', type=int, default=5, help='Maximum number of checkpoints to keep')\n",
    "    parser.add_argument('--load_best_model_at_end', action='store_true', help='Load the best model at the end of training')\n",
    "    parser.add_argument('--greater_is_better', action='store_true', help='Determines if a greater metric signifies a better model')\n",
    "\n",
    "    # Output arguments\n",
    "    parser.add_argument('--output_dir', type=str, default='output', help='Output directory')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_path = 'nikraf/uniref128-256AA'\n",
    "    tokenizer_path = 'facebook/esm2_t30_150M_UR50D'\n",
    "    model_path = 'facebook/esm2_t6_8M_UR50D'\n",
    "    save_path = './best_model'\n",
    "    weight_path = './best_model'\n",
    "    hidden_size = 512\n",
    "    num_hidden_layers = 24\n",
    "    seed = 338\n",
    "    wandb = True\n",
    "    batch_size = 1\n",
    "    num_epochs = 1\n",
    "    lr = 1e-4\n",
    "    mlm_probability = 'mixed'\n",
    "    patience = 3\n",
    "    group_by_length = True\n",
    "    grad_accum = 1\n",
    "    evaluation_strategy = 'steps'\n",
    "    lr_scheduler_type = 'cosine'\n",
    "    optim = 'adamw_torch'\n",
    "    log_path = './mlmlog.txt'\n",
    "    logging_steps = 10\n",
    "    eval_steps = 50\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 10\n",
    "    effective_batch_size = 1000000\n",
    "    valid_size = 5000\n",
    "    max_length = 512\n",
    "    fp16 = False\n",
    "    save_total_limit = 5\n",
    "    load_best_model_at_end = True\n",
    "    greater_is_better = True\n",
    "    output_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(args.data_path)['train']\n",
    "\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.seqs = dataset['seqs']\n",
    "        self.lengths = [len(seq) for seq in self.seqs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __avg__(self):\n",
    "        return sum(self.lengths) / len(self.lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        return seq\n",
    "    \n",
    "train_dataset = Dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsmForMLM(EsmForMaskedLM):\n",
    "\n",
    "    def __init__(self, model_path, hidden_size, num_hidden_layers, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        self.model_path = model_path\n",
    "        config = EsmConfig.from_pretrained(model_path)\n",
    "        config.hidden_size = hidden_size\n",
    "        config.num_hidden_layers = num_hidden_layers\n",
    "        config.num_attention_heads = config.hidden_size // 32\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForMixedMLM(DataCollatorMixin):\n",
    "    \n",
    "    def __init__(self, tokenizer: EsmTokenizer, mlm: bool = True, return_tensors: str = 'pt', pad_to_multiple_of: Optional[int] = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.return_tensors = return_tensors\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "\n",
    "    def torch_call(self, input): # input is list of examples from dataset\n",
    "        batch = self.tokenizer(input, return_tensors='pt', padding='longest', truncation=False, add_special_tokens=False)\n",
    "        if self.mlm:\n",
    "            batch['input_ids'], labels = self.torch_mask_tokens(batch['input_ids'])\n",
    "            batch['labels'] = labels # the keys here, need to match the keys in the model\n",
    "        return batch # in here are input_ids, attention_mask, and labels as keys in dictionary\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 90% MASK, 5% random, 5% original.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        #probability_matrix = torch.full(labels.shape, random.uniform(self.min_prob, self.max_prob))\n",
    "        probability_matrix = torch.normal(mean=0.3, std=0.12, size=labels.shape)\n",
    "        probability_matrix = torch.clamp(probability_matrix, min=0.0, max=1.0)\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "        # 85% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.90)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced # 0.5 because half of remaining are random\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "        # The rest of the time (5% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    \n",
    "\n",
    "class DataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability, **kwargs)\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.9)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Batching Summary-----\n",
      "\n",
      "Number of devices: 1\n",
      "Average sequence length: 184.8241525771167\n",
      "Local batch size: 1 seqs\n",
      "Gradient accumulation: 5410\n",
      "Effective batch size: 1000000 tokens\n"
     ]
    }
   ],
   "source": [
    "if args.effective_batch_size > args.batch_size: # effective batch_size is in tokens\n",
    "# for MLM, probably 1e6 tokens and learning rate of 1e-4 is good\n",
    "# or 1e5 tokens and lr of 1e-5\n",
    "    num_devices = torch.cuda.device_count() if torch.cuda.device_count() > 1 else 1 # for cpu\n",
    "    avg_length = train_dataset.__avg__()\n",
    "    args.grad_accum = int((args.effective_batch_size / avg_length) / (args.batch_size * num_devices))\n",
    "    args.grad_accum = args.grad_accum if args.grad_accum > 0 else 1\n",
    "    if args.grad_accum == 1:\n",
    "        args.effective_batch_size = avg_length * args.batch_size * num_devices\n",
    "\n",
    "    print('\\n-----Batching Summary-----\\n')\n",
    "    print(f'Number of devices: {num_devices}')\n",
    "    print(f'Average sequence length: {avg_length}')\n",
    "    print(f'Local batch size: {args.batch_size} seqs')\n",
    "    print(f'Gradient accumulation: {args.grad_accum}')\n",
    "    print(f'Effective batch size: {int(args.effective_batch_size)} tokens')\n",
    "else:\n",
    "    args.grad_accum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    logits = np.array(logits)\n",
    "    labels = np.array(labels)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    valid_indices = (labels != -100)\n",
    "    valid_preds = preds[valid_indices]\n",
    "    valid_labels = labels[valid_indices]\n",
    "    accuracy = np.mean(valid_preds == valid_labels)\n",
    "    return {'mlm_accuracy': accuracy}\n",
    "\n",
    "def log_metrics(config, metrics, header=None): # need a log_path in args, needs to be txt file\n",
    "    def log_nested_dict(d, parent_key=''):\n",
    "        filtered_results = {}\n",
    "        for k, v in d.items():\n",
    "            new_key = f'{parent_key}_{k}' if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                filtered_results.update(log_nested_dict(v, new_key))\n",
    "            elif 'runtime' not in k or 'second' not in k:\n",
    "                filtered_results[new_key] = round(v, 5) if isinstance(v, (float, int)) else v\n",
    "        return filtered_results\n",
    "\n",
    "    filtered_results = log_nested_dict(metrics)\n",
    "\n",
    "    with open(config.log_path, 'a') as f:\n",
    "        if header is not None:\n",
    "            f.write(header + '\\n')\n",
    "        for k, v in filtered_results.items():\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_collator(mlm_probability, tokenizer):\n",
    "\n",
    "    if mlm_probability == 'mixed':\n",
    "        return DataCollatorForMixedMLM(tokenizer=tokenizer)\n",
    "    else:\n",
    "        mlm_probability = float(mlm_probability)\n",
    "        return DataCollatorForMLM(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "\n",
    "model = EsmForMLM(args.model_path,hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, seed=args.seed)\n",
    "tokenizer = EsmTokenizer.from_pretrained(args.tokenizer_path)\n",
    "data_collator = get_data_collator(args.mlm_probability, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikraf99\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c03b762580427e9249823227b66f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nikra\\OneDrive\\Documents\\GitHub\\new_pretraining_tasks\\Phase 1\\wandb\\run-20240314_163531-b27q3uoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikraf99/huggingface/runs/b27q3uoz' target=\"_blank\">currant-pastry-7</a></strong> to <a href='https://wandb.ai/nikraf99/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikraf99/huggingface' target=\"_blank\">https://wandb.ai/nikraf99/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikraf99/huggingface/runs/b27q3uoz' target=\"_blank\">https://wandb.ai/nikraf99/huggingface/runs/b27q3uoz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d1efc6a4994aa5aa3f64cc0012dc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 56\u001b[0m\n\u001b[0;32m     20\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     21\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mwandb \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     group_by_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     46\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SortedTrainer(\n\u001b[0;32m     47\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     48\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mpatience)]  \u001b[38;5;66;03m# usually 3 - 5\u001b[39;00m\n\u001b[0;32m     54\u001b[0m )\n\u001b[1;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:1966\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1966\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SortedTrainer(Trainer):\n",
    "    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n",
    "        if self.train_dataset is None or not has_length(self.train_dataset):\n",
    "            return None\n",
    "        if self.args.group_by_length:\n",
    "            if is_datasets_available() and isinstance(self.train_dataset, TorchDataset):\n",
    "                lengths = self.train_dataset.lengths # this requires your dataset has a self.lengths with the lengths in it\n",
    "            else:\n",
    "                lengths = None\n",
    "            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n",
    "            return LengthGroupedSampler(\n",
    "                self.args.train_batch_size * self.args.gradient_accumulation_steps,\n",
    "                dataset=self.train_dataset,\n",
    "                lengths=lengths,\n",
    "                model_input_name=model_input_name,\n",
    "            )\n",
    "        else:\n",
    "            return RandomSampler(self.train_dataset)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to='wandb' if args.wandb else None,\n",
    "    output_dir=args.save_path,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=args.grad_accum,\n",
    "    logging_steps=args.logging_steps,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=args.eval_steps,\n",
    "    num_train_epochs=args.num_epochs,\n",
    "    weight_decay=args.weight_decay,\n",
    "    warmup_steps=args.warmup_steps,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=args.lr,\n",
    "    optim='adamw_torch',\n",
    "    seed=args.seed,\n",
    "    data_seed=args.seed,\n",
    "    save_steps=args.eval_steps,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    fp16=args.fp16,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SortedTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=args.patience)]  # usually 3 - 5\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
