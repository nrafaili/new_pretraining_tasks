data_config:
  data_path: !!str "nikraf/uniref128-256AA"
  tokenizer_path: !!str "facebook/esm2_t30_150M_UR50D"

model_config:
  hidden_size: !!int 512
  num_hidden_layers: !!int 24
  model_path: !!str "facebook/esm2_t30_150M_UR50D"

training_args:
  save_path: !!str "./best_model"
  weight_path: !!str "./best_model"
  wandb: !!bool true
  batch_size: !!int 32
  num_epochs: !!int 1
  learning_rate: !!float 0.0001
  mlm_probability: !!str "mixed"
  patience: !!int 3
  group_by_length: !!bool true
  grad_accum: !!int 1
  evaluation_strategy: !!str "steps"
  lr_scheduler_type: !!str "cosine"
  optim: !!str "adamw_torch"
  log_path: !!str "path/to/your/log"
  logging_steps: !!int 10
  eval_steps: !!int 50
  weight_decay: !!float 0.01
  warmup_steps: !!int 10
  effective_batch_size: !!int 1000000
  save_total_limit: !!int 5
  load_best_model_at_end: !!bool true
  greater_is_better: !!bool false
  seed: !!int 338

output_config:
  output_dir: !!str "output"
