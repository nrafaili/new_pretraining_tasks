{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction, TrainingArguments, Trainer, EarlyStoppingCallback, EsmForMaskedLM, DataCollatorForLanguageModeling, EsmTokenizer\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['seqs'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('nikraf/uniref128-256AA')\n",
    "dataset = dataset['train'].select(range(15))\n",
    "print(dataset)\n",
    "class Dataset(TorchDataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.seqs = dataset['seqs']\n",
    "        self.lengths = [len(seq) for seq in self.seqs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __avg__(self):\n",
    "        return sum(self.lengths) / len(self.lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        return {'seqs': seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForMaskedLM were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized because the shapes did not match:\n",
      "- esm.embeddings.word_embeddings.weight: found shape torch.Size([33, 640]) in the checkpoint and torch.Size([53, 640]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([33]) in the checkpoint and torch.Size([53]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def initialize_tokenizer():\n",
    "    tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t30_150M_UR50D')\n",
    "    new_tokens =  {'additional_special_tokens': []}\n",
    "\n",
    "    AA_tokens = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    shuffle_tokens = [f'<{AA}s>' for AA in AA_tokens]\n",
    "    new_tokens['additional_special_tokens'].extend(shuffle_tokens)\n",
    "\n",
    "    #Update tokenizer\n",
    "    if new_tokens['additional_special_tokens']:\n",
    "        total_tokens = len(tokenizer) + len(list(new_tokens.values())[0])\n",
    "        tokenizer.add_special_tokens(new_tokens)\n",
    "\n",
    "    return tokenizer, total_tokens, new_tokens\n",
    "\n",
    "tokenizer, total_tokens, new_tokens = initialize_tokenizer()\n",
    "model = EsmForMaskedLM.from_pretrained('facebook/esm2_t30_150M_UR50D', vocab_size=total_tokens, ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.resize_token_embeddings(total_tokens)\n",
    "    try:\n",
    "        cls_token_embedding = model.embeddings.word_embeddings.weight[tokenizer.cls_token_id, :].clone()\n",
    "        for token in new_tokens['additional_special_tokens']:\n",
    "            model.embeddings.word_embeddings.weight[tokenizer._convert_token_to_id(token), :] = cls_token_embedding.clone()\n",
    "    except AttributeError:\n",
    "        cls_token_embedding = model.esm.embeddings.word_embeddings.weight[tokenizer.cls_token_id, :].clone()\n",
    "        for token in new_tokens['additional_special_tokens']:\n",
    "            model.esm.embeddings.word_embeddings.weight[tokenizer._convert_token_to_id(token), :] = cls_token_embedding.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForShuffling(DataCollatorForLanguageModeling):\n",
    "\n",
    "    def __init__(self, tokenizer: EsmTokenizer, shuffle_type: str = 'regular', **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, **kwargs)\n",
    "        self.return_tensors = 'pt'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.shuffle_type = shuffle_type\n",
    "\n",
    "    def shuffle_seq(self, seq):\n",
    "        if self.shuffle_type == 'sectional':\n",
    "            return self.sectional_shuffle(seq)\n",
    "        elif self.shuffle_type == 'regular':\n",
    "            return self.regular_shuffle(seq)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid shuffle_type. Choose 'sectional' or 'regular'.\")\n",
    "    '''\n",
    "\n",
    "PREVIOUS APPROACH -- DID NOT WORK BECAUSE WE WERE PICKING FROM set(seq), see new approach below\n",
    "    def regular_shuffle(self, seq):\n",
    "        seq = list(seq)\n",
    "        #per = np.clip(np.random.normal(0.3, 0.12), 0.0, 1.0)\n",
    "        per = 0.3 \n",
    "        seq_len = len(seq)\n",
    "        num_to_shuffle = int(seq_len * per)\n",
    "        \n",
    "        shuffled_indices = random.sample(range(seq_len), num_to_shuffle)\n",
    "        new_tokens = []\n",
    "\n",
    "        for index in shuffled_indices:\n",
    "            original_char = seq[index]\n",
    "            possible_tokens = [f\"<{token}s>\" for token in set(seq) - {original_char}]\n",
    "            new_token = random.choice(possible_tokens)\n",
    "            new_tokens.append(new_token)\n",
    "\n",
    "\n",
    "        random.shuffle(new_tokens)\n",
    "\n",
    "        for new_token, seq_index in zip(new_tokens, shuffled_indices):\n",
    "            seq[seq_index] = new_token\n",
    "        \n",
    "        return ''.join(seq)\n",
    "    '''\n",
    "\n",
    "    def regular_shuffle(self, seq):\n",
    "        original_seq = list(seq)  # Save the original sequence\n",
    "\n",
    "        seq = list(seq)\n",
    "        per = np.clip(np.random.normal(0.3, 0.12), 0.0, 1.0)\n",
    "        seq_len = len(seq)\n",
    "        num_to_shuffle = int(seq_len * per)\n",
    "        \n",
    "        shuffled_indices = random.sample(range(seq_len), num_to_shuffle)\n",
    "        shuffled_indices_set = set(shuffled_indices)\n",
    "        \n",
    "        index_mapping = {}\n",
    "        new_tokens = []\n",
    "\n",
    "        for original_index in shuffled_indices:\n",
    "            # Pick a unique index, exclude original\n",
    "            new_index = random.choice(list(shuffled_indices_set - {original_index}))\n",
    "            shuffled_indices_set.remove(new_index)  # Remove picked index to prevent reuse\n",
    "            \n",
    "            # Mapping of original index to new index\n",
    "            index_mapping[original_index] = new_index\n",
    "\n",
    "            # Add <s> \n",
    "            new_token = f\"<{seq[new_index]}s>\"\n",
    "            new_tokens.append((new_token, original_index))\n",
    "\n",
    "        # Shuffle\n",
    "            \n",
    "        random.shuffle(new_tokens)\n",
    "\n",
    "        # Replace with new tokens\n",
    "        for new_token, original_index in new_tokens:\n",
    "            seq[original_index] = new_token\n",
    "        \n",
    "        sorted_index_mapping = dict(sorted(index_mapping.items(), key=lambda item: item[1]))\n",
    "        print(sorted_index_mapping)\n",
    "        amino_acid_pairs = [(original_seq[original], seq[new]) \n",
    "                            for original, new in sorted_index_mapping.items()]\n",
    "\n",
    "        print(\"Amino Acid Pairs:\")\n",
    "        for pair in amino_acid_pairs:\n",
    "            print(pair)\n",
    "\n",
    "        return ''.join(seq)\n",
    "\n",
    "    def sectional_shuffle(self, seq):\n",
    "        original_seq = list(seq)\n",
    "        seq_len = len(original_seq)\n",
    "        per = np.clip(np.random.normal(0.3, 0.12), 0.0, 1.0)\n",
    "        section_length = int(seq_len * per)\n",
    "\n",
    "        start_index = random.randrange(seq_len)\n",
    "        end_index = (start_index + section_length) % seq_len\n",
    "\n",
    "        if end_index > start_index:\n",
    "            shuffle_section = original_seq[start_index:end_index]\n",
    "        else:  \n",
    "            shuffle_section = original_seq[start_index:] + original_seq[:end_index]\n",
    "\n",
    "        new_section_tokens = []\n",
    "        all_tokens = set(shuffle_section)\n",
    "\n",
    "        for index in shuffle_section:       \n",
    "            possible_tokens = [f\"<{token}s>\" for token in all_tokens - {index}]\n",
    "            new_section_token = random.choice(possible_tokens)\n",
    "            new_section_tokens.append(new_section_token)\n",
    "\n",
    "        random.shuffle(new_section_tokens)\n",
    "\n",
    "        \n",
    "        if end_index > start_index:\n",
    "            original_seq[start_index:end_index] = new_section_tokens\n",
    "        else:\n",
    "            original_seq[start_index:] = new_section_tokens[:len(original_seq) - start_index]\n",
    "            original_seq[:end_index] = new_section_tokens[len(original_seq) - start_index:]\n",
    "\n",
    "        return ''.join(original_seq)\n",
    "\n",
    "    def torch_call(self, seqs):\n",
    "        shuffled_seqs = [self.shuffle_seq(seq) for seq in seqs]\n",
    "\n",
    "        labels = self.tokenizer(seqs, return_tensors=self.return_tensors, padding='longest', truncation=False).input_ids\n",
    "\n",
    "        tokens = self.tokenizer(shuffled_seqs, return_tensors=self.return_tensors, padding='longest', truncation=False, return_token_type_ids=False)\n",
    "\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        tokens['labels'] = labels\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{91: 2, 131: 37, 63: 39, 75: 40, 2: 61, 123: 63, 61: 67, 165: 68, 141: 70, 145: 73, 70: 75, 156: 81, 97: 86, 102: 91, 73: 97, 40: 101, 106: 102, 86: 106, 172: 117, 124: 123, 101: 124, 37: 131, 117: 141, 39: 145, 81: 146, 68: 156, 146: 165, 67: 172}\n",
      "Amino Acid Pairs:\n",
      "('G', '<Gs>')\n",
      "('I', '<Is>')\n",
      "('G', '<Es>')\n",
      "('R', '<Gs>')\n",
      "('E', '<Ds>')\n",
      "('R', '<Rs>')\n",
      "('G', '<Es>')\n",
      "('E', '<Es>')\n",
      "('E', '<Rs>')\n",
      "('E', '<Ss>')\n",
      "('D', '<Es>')\n",
      "('E', '<Es>')\n",
      "('S', '<Gs>')\n",
      "('R', '<Es>')\n",
      "('G', '<Ks>')\n",
      "('E', '<Gs>')\n",
      "('G', '<Gs>')\n",
      "('K', '<Rs>')\n",
      "('E', '<Es>')\n",
      "('G', '<Gs>')\n",
      "('G', '<Rs>')\n",
      "('R', '<Rs>')\n",
      "('D', '<Ds>')\n",
      "('R', '<Gs>')\n",
      "('Y', '<Es>')\n",
      "('R', '<Ys>')\n",
      "('E', '<Rs>')\n",
      "('D', '<Ds>')\n",
      "{78: 0, 103: 1, 70: 2, 42: 3, 1: 4, 100: 6, 141: 7, 40: 8, 87: 11, 128: 12, 115: 15, 36: 16, 55: 17, 94: 18, 32: 19, 16: 20, 91: 21, 81: 22, 113: 24, 17: 26, 126: 27, 101: 29, 97: 30, 24: 32, 86: 35, 60: 36, 111: 37, 52: 38, 35: 40, 106: 41, 27: 42, 7: 43, 82: 44, 112: 45, 0: 49, 125: 50, 119: 51, 142: 52, 45: 53, 20: 55, 123: 58, 73: 59, 76: 60, 131: 61, 109: 62, 147: 64, 93: 65, 18: 66, 133: 68, 88: 70, 41: 71, 122: 72, 6: 73, 22: 75, 124: 76, 118: 78, 43: 81, 53: 82, 121: 83, 44: 85, 61: 86, 134: 87, 83: 88, 29: 89, 64: 90, 90: 91, 65: 93, 58: 94, 136: 95, 144: 96, 8: 97, 89: 100, 135: 101, 137: 103, 12: 105, 59: 106, 130: 107, 15: 109, 26: 110, 37: 111, 138: 112, 75: 113, 49: 115, 51: 116, 120: 118, 72: 119, 21: 120, 68: 121, 50: 122, 107: 123, 62: 124, 3: 125, 71: 126, 143: 128, 132: 130, 2: 131, 11: 132, 38: 133, 95: 134, 96: 135, 30: 136, 116: 137, 85: 138, 66: 141, 110: 142, 4: 143, 105: 144, 19: 147}\n",
      "Amino Acid Pairs:\n",
      "('Q', '<Rs>')\n",
      "('T', '<Rs>')\n",
      "('N', '<Ss>')\n",
      "('E', '<Is>')\n",
      "('N', '<Is>')\n",
      "('V', '<Gs>')\n",
      "('G', '<Ss>')\n",
      "('P', '<Ls>')\n",
      "('G', '<Ts>')\n",
      "('E', '<Gs>')\n",
      "('V', '<Ks>')\n",
      "('K', '<Is>')\n",
      "('R', '<As>')\n",
      "('D', '<Es>')\n",
      "('H', '<Fs>')\n",
      "('T', '<Rs>')\n",
      "('I', '<Ks>')\n",
      "('G', '<Ys>')\n",
      "('N', '<Hs>')\n",
      "('L', '<Vs>')\n",
      "('V', '<Es>')\n",
      "('S', '<Ls>')\n",
      "('L', '<Ts>')\n",
      "('I', '<As>')\n",
      "('I', '<Ps>')\n",
      "('I', '<Ts>')\n",
      "('K', '<Ks>')\n",
      "('V', '<Is>')\n",
      "('G', '<Ss>')\n",
      "('K', '<Fs>')\n",
      "('Q', '<Ss>')\n",
      "('L', '<Gs>')\n",
      "('G', '<Ns>')\n",
      "('Y', '<Qs>')\n",
      "('M', '<Vs>')\n",
      "('I', '<Ls>')\n",
      "('R', '<Ks>')\n",
      "('S', '<Vs>')\n",
      "('E', '<Gs>')\n",
      "('I', '<Ls>')\n",
      "('E', '<Ds>')\n",
      "('G', '<Ks>')\n",
      "('V', '<Ks>')\n",
      "('S', '<Is>')\n",
      "('K', '<Ss>')\n",
      "('F', '<Ss>')\n",
      "('S', '<Ss>')\n",
      "('S', '<Gs>')\n",
      "('I', '<Ss>')\n",
      "('F', '<Ls>')\n",
      "('I', '<Vs>')\n",
      "('L', '<Rs>')\n",
      "('L', '<Ls>')\n",
      "('S', '<Ns>')\n",
      "('S', '<Is>')\n",
      "('S', '<Ms>')\n",
      "('S', '<Ss>')\n",
      "('Q', '<Es>')\n",
      "('S', '<Fs>')\n",
      "('E', '<Ds>')\n",
      "('K', '<Gs>')\n",
      "('D', '<Vs>')\n",
      "('V', '<Ns>')\n",
      "('K', '<Vs>')\n",
      "('E', '<Is>')\n",
      "('S', '<Ps>')\n",
      "('N', '<Ns>')\n",
      "('K', '<Ss>')\n",
      "('T', '<Ds>')\n",
      "('S', '<Es>')\n",
      "('S', '<Es>')\n",
      "('L', '<Ls>')\n",
      "('E', '<Ks>')\n",
      "('D', '<Ns>')\n",
      "('I', '<Ss>')\n",
      "('L', '<Is>')\n",
      "('S', '<Es>')\n",
      "('L', '<Fs>')\n",
      "('A', '<Ss>')\n",
      "('Y', '<Ys>')\n",
      "('D', '<Es>')\n",
      "('Y', '<Is>')\n",
      "('R', '<Ls>')\n",
      "('E', '<Ds>')\n",
      "('K', '<Qs>')\n",
      "('A', '<Es>')\n",
      "('P', '<Ss>')\n",
      "('E', '<Vs>')\n",
      "('I', '<As>')\n",
      "('D', '<Ks>')\n",
      "('F', '<Vs>')\n",
      "('S', '<Is>')\n",
      "('F', '<Qs>')\n",
      "('I', '<Is>')\n",
      "('T', '<Ds>')\n",
      "('L', '<Ks>')\n",
      "('V', '<Ss>')\n",
      "('V', '<Es>')\n",
      "('D', '<Gs>')\n",
      "('Q', '<Ss>')\n",
      "('E', '<Ds>')\n",
      "('K', '<Ts>')\n",
      "('N', '<Ys>')\n",
      "('E', '<Ls>')\n",
      "('V', '<Vs>')\n",
      "('R', '<Es>')\n",
      "('G', '<Qs>')\n",
      "('A', '<Es>')\n",
      "{197: 2, 16: 9, 51: 11, 102: 12, 127: 15, 110: 16, 171: 19, 44: 22, 185: 23, 188: 28, 118: 37, 55: 38, 77: 44, 99: 48, 196: 51, 63: 54, 142: 55, 68: 61, 181: 62, 159: 63, 133: 64, 11: 68, 61: 71, 165: 75, 19: 77, 54: 83, 104: 85, 83: 86, 160: 92, 203: 98, 169: 99, 126: 101, 117: 102, 75: 104, 166: 107, 38: 110, 12: 117, 124: 118, 148: 124, 48: 126, 64: 127, 86: 130, 206: 133, 23: 142, 28: 148, 62: 149, 149: 159, 15: 160, 22: 165, 92: 166, 98: 169, 101: 171, 37: 181, 85: 185, 130: 188, 9: 196, 2: 197, 107: 203, 71: 206}\n",
      "Amino Acid Pairs:\n",
      "('F', '<Fs>')\n",
      "('S', '<Ks>')\n",
      "('E', '<Fs>')\n",
      "('T', '<Hs>')\n",
      "('P', '<As>')\n",
      "('Y', '<Is>')\n",
      "('E', '<Ks>')\n",
      "('Q', '<Ks>')\n",
      "('T', '<Gs>')\n",
      "('T', '<Ds>')\n",
      "('P', '<As>')\n",
      "('R', '<Ys>')\n",
      "('K', '<Ls>')\n",
      "('Y', '<Ls>')\n",
      "('K', '<Ls>')\n",
      "('V', '<Ls>')\n",
      "('G', '<Is>')\n",
      "('F', '<Ls>')\n",
      "('A', '<Ps>')\n",
      "('V', '<Ts>')\n",
      "('S', '<Ps>')\n",
      "('L', '<Ps>')\n",
      "('P', '<Ps>')\n",
      "('K', '<As>')\n",
      "('K', '<Qs>')\n",
      "('T', '<Ss>')\n",
      "('A', '<Ts>')\n",
      "('L', '<Is>')\n",
      "('A', '<As>')\n",
      "('K', '<Hs>')\n",
      "('H', '<Ds>')\n",
      "('L', '<Es>')\n",
      "('H', '<Ss>')\n",
      "('T', '<Ss>')\n",
      "('A', '<Ks>')\n",
      "('I', '<Ss>')\n",
      "('S', '<Ts>')\n",
      "('Y', '<Ds>')\n",
      "('D', '<Ps>')\n",
      "('D', '<Ss>')\n",
      "('V', '<Vs>')\n",
      "('S', '<Ts>')\n",
      "('P', '<Vs>')\n",
      "('G', '<Rs>')\n",
      "('D', '<Ys>')\n",
      "('L', '<Vs>')\n",
      "('P', '<Vs>')\n",
      "('V', '<Ps>')\n",
      "('L', '<Ts>')\n",
      "('P', '<Ls>')\n",
      "('L', '<Ys>')\n",
      "('S', '<Ks>')\n",
      "('D', '<Ls>')\n",
      "('S', '<Gs>')\n",
      "('I', '<Ds>')\n",
      "('I', '<Es>')\n",
      "('E', '<Es>')\n",
      "('L', '<Ls>')\n",
      "('L', '<Ss>')\n",
      "\n",
      "Decoded Shuffled Sequences (from label IDs):\n",
      "<cls> M E <Gs> E K K I N E I E E E E E E E I K E E K I E E E E P E R R R R G W R R <Is> Q <Es> <Gs> I K K G G G D R G G D R R R E E D I G E <Ds> G <Rs> R R G <Es> <Es> G <Rs> K G <Ss> D <Es> R G R R C <Es> R R G D <Gs> R G R K <Es> G G D K G <Ks> G D K <Gs> <Gs> E R G <Rs> D K R N C G G G R R <Es> K G G E G <Gs> <Rs> R V D K G E <Rs> K E E E E E I K D <Ds> E K I <Gs> <Es> I K E E I K E E K <Ys> I K E K E I E E <Rs> E K E E I E <Ds> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<cls> <Rs> <Rs> <Ss> <Is> <Is> K <Gs> <Ss> <Ls> L F <Ts> <Gs> G A <Ks> <Is> <As> <Es> <Fs> <Rs> <Ks> <Ys> Q <Hs> Q <Vs> <Es> T <Ls> <Ts> S <As> S Q <Ps> <Ts> <Ks> <Is> T <Ss> <Fs> <Ss> <Gs> <Ns> <Qs> F W K <Vs> <Ls> <Ks> <Vs> <Gs> E <Ls> I P <Ds> <Ks> <Ks> <Is> <Ss> L <Ss> <Ss> <Gs> F <Ss> G <Ls> <Vs> <Rs> <Ls> L <Ns> <Is> D <Ms> E N <Ss> <Es> <Fs> I <Ds> <Gs> <Vs> <Ns> <Vs> <Is> <Ps> P <Ns> <Ss> <Ds> <Es> <Es> I E <Ls> <Ks> K <Ns> L <Ss> <Is> <Es> V <Fs> <Ss> <Ys> <Es> <Is> E <Ls> <Ds> Y <Qs> <Es> <Ss> <Vs> <As> <Ks> <Vs> <Is> <Qs> D <Is> L <Ds> <Ks> <Ss> <Es> <Gs> <Ss> <Ds> <Ts> <Ys> E I <Ls> <Vs> <Es> <Qs> S S <Es> A <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<cls> F Q <Fs> W G G C D T <Ks> I <Fs> <Hs> A G <As> <Is> A L <Ks> P I <Ks> <Gs> I A G A <Ds> P E H S R E L A <As> <Ys> D K D G V <Ls> R T L <Ls> A T <Ls> A A <Ls> <Is> A N Y Y G <Ls> <Ps> <Ts> <Ps> V T A <Ps> L P <Ps> L T R <As> S <Qs> S P S I L <Ss> V <Ts> <Is> L G A L I <As> C P T R A <Hs> <Ds> G <Es> <Ss> K <Ss> A S <Ks> I M <Ss> Q A M S V E <Ts> <Ds> G V A F S <Ps> V <Ss> <Vs> T T <Ts> E G <Vs> F R A S A V D Q <Rs> A V R E S <Ys> <Vs> N R S G L K V G A <Vs> <Ps> A R C L <Ts> <Ls> V D <Ys> G <Ks> R T V F M P G A M <Ls> I L P <Gs> L Y <Ds> F L P S L V D <Es> <Es> T R K K Y <Ls> Y H <Ss> V V <eos>\n",
      "\n",
      "Decoded Original Sequences (from label IDs):\n",
      "<cls> M E E E K K I N E I E E E E E E E I K E E K I E E E E P E R R R R G W R R R Q R E I K K G G G D R G G D R R R E E D I G E G G G R R G D R G D K G G D R R G R R C Y R R G D K R G R K G G G D K G S G D K G R E R G G D K R N C G G G R R D K G G E G R G R V D K G E I K E E E E E I K D E E K I E E I K E E I K E E K E I K E K E I E E E E K E E I E E <eos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<cls> M N L S R K L L S L F V I G A L T L S A I P S Q I Q A Q T K E S H S Q G K Y V T P I E S E E F W K R I E V Q E R I P K L I K F L E N E F E G N F A G L Y V D Q E N G G V I N I G F L S I P S D D Q L I E V S K T L G K D V K V K Y N E V K Y S R K S L E S I V D E L S S T I D E T D D E I G S I S S S F A <eos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "<cls> F Q E W G G C D T I I L S A G V S A L K P I L G I A G A D P E H S R E L A D I D K D G V Q R T L D A T E A A T R A N Y Y G P L V V V T A F L P L L T R T S K S P S I L L V S S L G A L I P C P T R A L Y G S T K A A S L I M Y Q A M S V E H P G V A F S Y V L P T T I E G S F R A S A V D Q G A V R E S D P N R S G L K V G A V A A R C L K A V D H G E R T V F M P G A M A I L P T L Y T F L P S L V D K F T R K K Y K Y H P V V <eos>\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForShuffling(return_tensors='pt', tokenizer=tokenizer)\n",
    "\n",
    "torch_dataset = Dataset(dataset)\n",
    "examples = [torch_dataset[i] for i in range(3)]\n",
    "batch = data_collator.torch_call([example['seqs'] for example in examples])\n",
    "\n",
    "decoded_shuffled_seqs = [tokenizer.decode(ids, skip_special_tokens=False) for ids in batch['input_ids']]\n",
    "decoded_original_seqs = [tokenizer.decode(ids, skip_special_tokens=False) for ids in batch['labels']]\n",
    "\n",
    "print(\"\\nDecoded Shuffled Sequences (from label IDs):\")\n",
    "for seq in decoded_shuffled_seqs:\n",
    "    print(seq)\n",
    "\n",
    "print(\"\\nDecoded Original Sequences (from label IDs):\")\n",
    "for seq in decoded_original_seqs:\n",
    "    print(seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
